{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the SimCLR baseline based on the original code available on\n",
    "https://github.com/sthalles/SimCLR\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "os.chdir(\"../\") #Load from parent directory\n",
    "from data_utils import Plots,gen_loader,load_datasets,split_series,CustomTensorDataset\n",
    "from models import select_encoder\n",
    "utils_plot=Plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTXentLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, device, batch_size, temperature, use_cosine_similarity):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
    "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_similarity_function(self, use_cosine_similarity=True):\n",
    "        if use_cosine_similarity:\n",
    "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "            return self._cosine_simililarity\n",
    "        else:\n",
    "            return self._dot_simililarity\n",
    "\n",
    "    def _get_correlated_mask(self):\n",
    "        diag = np.eye(2 * self.batch_size)\n",
    "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
    "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
    "        mask = torch.from_numpy((diag + l1 + l2))\n",
    "        mask = (1 - mask).type(torch.bool)\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dot_simililarity(x, y):\n",
    "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, C, 2N)\n",
    "        # v shape: (N, 2N)\n",
    "        return v\n",
    "\n",
    "    def _cosine_simililarity(self, x, y):\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, 2N, C)\n",
    "        # v shape: (N, 2N)\n",
    "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
    "        return v\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        zis = nn.functional.normalize(zis, dim=1)\n",
    "        zjs = nn.functional.normalize(zjs, dim=1)\n",
    "        \n",
    "        representations = torch.cat([zjs, zis], dim=0)\n",
    "        similarity_matrix = self.similarity_function(representations, representations)\n",
    "        #print(similarity_matrix)\n",
    "        # filter out the scores from the positive samples\n",
    "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
    "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        \n",
    "        #print(l_pos.shape,r_pos.shape,self.batch_size)\n",
    "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
    "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
    "        logits = torch.cat((positives, negatives), dim=1)\n",
    "        logits /= self.temperature\n",
    "\n",
    "        labels = torch.zeros(2 * self.batch_size).to(self.device).long()\n",
    "        \n",
    "        \n",
    "        loss = self.criterion(logits, labels)\n",
    "        return loss / (2 * self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simclr(device,device_ids,datasets,n_cross_val,verbose,lr,data_type,encoder_type,show_encodings,\n",
    "                 tr_percentage,window_size,batch_size,encoding_size,n_epochs,suffix):\n",
    "    \n",
    "    train_accs, test_accs = {},{}\n",
    "    train_losses, test_losses ={},{}\n",
    "    val_accs,val_losses ={},{}\n",
    "    \n",
    "    for cv in range(n_cross_val):\n",
    "        \n",
    "        #Save Location\n",
    "        save_dir = './results/baselines/%s_simclr/%s/'%(datasets,data_type)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        save_file = str((save_dir +'encoding_%d_encoder_%d_checkpoint_%d%s.pth.tar')\n",
    "               %(encoding_size,encoder_type, cv,suffix))\n",
    "        \n",
    "        if verbose:\n",
    "            print('Saving at: ',save_file)\n",
    "            \n",
    "        best_loss = np.inf\n",
    "        \n",
    "        \n",
    "        train_data,train_labels,test_data,test_labels = load_datasets(data_type,datasets,cv)\n",
    "        x,y = split_series(train_data,train_labels,window_size)\n",
    "        \n",
    "        if batch_size<1:\n",
    "            batch_size = max(1,int(min(len(train_data),len(test_data))*batch_size))\n",
    "            print('Using batch_size:', batch_size)\n",
    "\n",
    "        nt_xent_criterion = NTXentLoss(device, batch_size,temperature = 0.5,use_cosine_similarity=True)\n",
    "        inds = list(range(len(x)))\n",
    "        random.shuffle(inds)\n",
    "        x = x[inds]\n",
    "        y = y[inds]\n",
    "        n_train = int(tr_percentage*len(x))\n",
    "        \n",
    "        sample_rate=250\n",
    "        trainset = CustomTensorDataset([x[:n_train],y[:n_train]],sample_rate=sample_rate,is_transform=True)\n",
    "        valset = CustomTensorDataset([x[n_train:],y[n_train:]],sample_rate=sample_rate,is_transform=True)\n",
    "        \n",
    "        sampler = torch.utils.data.sampler.BatchSampler(torch.utils.data.sampler.RandomSampler\n",
    "                                                (trainset),batch_size=batch_size,drop_last=True)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,sampler=sampler)\n",
    "        \n",
    "        sampler = torch.utils.data.sampler.BatchSampler(torch.utils.data.sampler.RandomSampler\n",
    "                                                (valset),batch_size=batch_size,drop_last=True)\n",
    "        val_loader = torch.utils.data.DataLoader(valset,sampler=sampler)\n",
    "        \n",
    "        input_size = train_data.shape[1]\n",
    "        encoder,_ = select_encoder(device,encoder_type,input_size,encoding_size)\n",
    "        encoder = encoder.to(device)\n",
    "        optimizer = torch.optim.Adam(encoder.parameters(), lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, n_epochs, gamma=0.99)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "            encoder.train()\n",
    "            for i,((xis, xjs),_) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                xis = xis.squeeze(0).to(device)\n",
    "                xjs = xjs.squeeze(0).to(device)\n",
    "\n",
    "                # get the representations and the projections\n",
    "                zis = encoder(xis.to(device))  # [N,C]\n",
    "                zjs = encoder(xjs.to(device))  # [N,C]\n",
    "                \n",
    "                #print(zis.shape,zjs.shape)\n",
    "                loss = nt_xent_criterion(zis, zjs)\n",
    "                total_loss+=loss.item()\n",
    "                batch_count += 1\n",
    "                loss.backward()\n",
    "                #print(loss)\n",
    "                optimizer.step()\n",
    "                \n",
    "            scheduler.step()\n",
    "            if verbose:\n",
    "                print('CV ',cv,' Epoch ',epoch,'Train Labels',tr_percentage)\n",
    "            \n",
    "                print('Train Results: ',total_loss / batch_count)\n",
    "            \n",
    "            val_loss = test_supervised(encoder,device,val_loader,batch_size,calc_auc=False)\n",
    "            if verbose:\n",
    "                print('Validation Results: ',val_loss)\n",
    "            if best_loss>=val_loss:\n",
    "                best_loss = val_loss\n",
    "                state = {\n",
    "                        'epoch': epoch,\n",
    "                        'encoder_state_dict': encoder.state_dict()\n",
    "                    }\n",
    "                torch.save(state, save_file)\n",
    "                if verbose:\n",
    "                    print('Saving ckpt')\n",
    "            if verbose:\n",
    "                print('')    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_supervised(encoder,device,data_loader,batch_size,calc_auc=False):\n",
    "    encoder.eval()\n",
    "    \n",
    "    nt_xent_criterion = NTXentLoss(device, batch_size,temperature = 0.5,use_cosine_similarity=True)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for i,((xis, xjs),_) in enumerate(data_loader):\n",
    "        xis = xis.squeeze(0).to(device)\n",
    "        xjs = xjs.squeeze(0).to(device)\n",
    "\n",
    "        # get the representations and the projections\n",
    "        zis = encoder(xis.to(device))  # [N,C]\n",
    "        zjs = encoder(xjs.to(device))  # [N,C]\n",
    "\n",
    "        #print(zis.shape,zjs.shape)\n",
    "        loss = nt_xent_criterion(zis, zjs)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "    return epoch_loss / batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simclr(args):\n",
    "\n",
    "    #Run Process\n",
    "    train_simclr(**args)\n",
    "    \n",
    "    #Plot Features\n",
    "    title = 'SimCLR Encoding TSNE for %s'%(args['data_type'])\n",
    "    \n",
    "    if args['show_encodings']:\n",
    "        \n",
    "        for cv in range(args['n_cross_val']):\n",
    "            _,_,test_data,test_labels = load_datasets(args['data_type'],args['datasets'],cv)\n",
    "            utils_plot.plot_distribution(test_data, test_labels,args['encoder_type'],\n",
    "                                                             args['encoding_size'],args['window_size'],'simclr',\n",
    "                                                             args['datasets'],args['data_type'],args['suffix'],\n",
    "                                                             args['device'], title, cv,parallel=False)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    #Devices\n",
    "    args['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    args['device_ids'] = [0]\n",
    "    print('Using', args['device'])\n",
    "\n",
    "    #Experiments\n",
    "    if args['data_type']=='afdb':\n",
    "        args['lr'] = 1e-3\n",
    "    if args['data_type']=='ims':\n",
    "        args['lr'] = 1e-3    \n",
    "    if args['data_type']=='urban':\n",
    "        args['n_cross_val'] = 10\n",
    "        args['lr'] = 1e-4   \n",
    "        \n",
    "    #Experiment Parameters\n",
    "    args['window_size'] = 2500\n",
    "    args['encoder_type'] = 1\n",
    "    args['encoding_size'] = 128\n",
    "    args['datasets'] = args['data_type']\n",
    "\n",
    "    run_simclr(args)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = {'n_cross_val':5,\n",
    "        'data_type':'afdb', #options: afdb, ims, urban\n",
    "        'tr_percentage':0.8,\n",
    "        'n_epochs':15,\n",
    "        'batch_size':16,\n",
    "        'suffix':'',\n",
    "        'show_encodings':False,\n",
    "        'verbose': True} \n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
