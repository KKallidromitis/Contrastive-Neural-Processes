{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the Triplet Loss baseline based on the original code available on\n",
    "https://github.com/White-Link/UnsupervisedScalableRepresentationLearningTimeSeries\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "os.chdir(\"../\") #Load from parent directory\n",
    "from data_utils import Plots,gen_loader,load_datasets\n",
    "from models import select_encoder\n",
    "utils_plot=Plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(torch.nn.modules.loss._Loss):\n",
    "    \"\"\"\n",
    "    Triplet loss for representations of time series. Optimized for training\n",
    "    sets where all time series have the same length.\n",
    "    Takes as input a tensor as the chosen batch to compute the loss,\n",
    "    a PyTorch module as the encoder, a 3D tensor (`B`, `C`, `L`) containing\n",
    "    the training set, where `B` is the batch size, `C` is the number of\n",
    "    channels and `L` is the length of the time series, as well as a boolean\n",
    "    which, if True, enables to save GPU memory by propagating gradients after\n",
    "    each loss term, instead of doing it after computing the whole loss.\n",
    "    The triplets are chosen in the following manner. First the size of the\n",
    "    positive and negative samples are randomly chosen in the range of lengths\n",
    "    of time series in the dataset. The size of the anchor time series is\n",
    "    randomly chosen with the same length upper bound but the the length of the\n",
    "    positive samples as lower bound. An anchor of this length is then chosen\n",
    "    randomly in the given time series of the train set, and positive samples\n",
    "    are randomly chosen among subseries of the anchor. Finally, negative\n",
    "    samples of the chosen length are randomly chosen in random time series of\n",
    "    the train set.\n",
    "    @param compared_length Maximum length of randomly chosen time series. If\n",
    "           None, this parameter is ignored.\n",
    "    @param nb_random_samples Number of negative samples per batch example.\n",
    "    @param negative_penalty Multiplicative coefficient for the negative sample\n",
    "           loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, compared_length, nb_random_samples, negative_penalty):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.compared_length = compared_length\n",
    "        if self.compared_length is None:\n",
    "            self.compared_length = np.inf\n",
    "        self.nb_random_samples = nb_random_samples\n",
    "        self.negative_penalty = negative_penalty\n",
    "\n",
    "    def forward(self, batch, encoder, train, save_memory=False):\n",
    "        batch_size = batch.size(0)\n",
    "        train_size = train.size(0)\n",
    "        length = min(self.compared_length, train.size(2))\n",
    "\n",
    "        # For each batch element, we pick nb_random_samples possible random\n",
    "        # time series in the training set (choice of batches from where the\n",
    "        # negative examples will be sampled)\n",
    "        samples = np.random.choice(\n",
    "            train_size, size=(self.nb_random_samples, batch_size)\n",
    "        )\n",
    "        samples = torch.LongTensor(samples)\n",
    "\n",
    "        # Choice of length of positive and negative samples\n",
    "        length_pos_neg = self.compared_length\n",
    "        # length_pos_neg = np.random.randint(1, high=length + 1)\n",
    "\n",
    "\n",
    "        # We choose for each batch example a random interval in the time\n",
    "        # series, which is the 'anchor'\n",
    "        random_length = self.compared_length\n",
    "\n",
    "        #print(length,random_length,batch_size)\n",
    "        beginning_batches = np.random.randint(\n",
    "            0, high=length - random_length + 1, size=batch_size\n",
    "        )  # Start of anchors\n",
    "\n",
    "        # The positive samples are chosen at random in the chosen anchors\n",
    "        beginning_samples_pos = np.random.randint(\n",
    "            0, high=random_length + 1, size=batch_size\n",
    "        )  # Start of positive samples in the anchors\n",
    "        # Start of positive samples in the batch examples\n",
    "        beginning_positive = beginning_batches + beginning_samples_pos\n",
    "        # End of positive samples in the batch examples\n",
    "        end_positive = beginning_positive + length_pos_neg + np.random.randint(0,self.compared_length)\n",
    "\n",
    "        # We randomly choose nb_random_samples potential negative samples for\n",
    "        # each batch example\n",
    "        beginning_samples_neg = np.random.randint(\n",
    "            0, high=length - length_pos_neg + 1,\n",
    "            size=(self.nb_random_samples, batch_size)\n",
    "        )\n",
    "\n",
    "        representation = encoder(torch.cat(\n",
    "            [batch[\n",
    "                j: j + 1, :,\n",
    "                beginning_batches[j]: beginning_batches[j] + random_length\n",
    "            ] for j in range(batch_size)]))  # Anchors representations        \n",
    "        ##\n",
    "        positive_representation = encoder(torch.cat(\n",
    "            [batch[\n",
    "                j: j + 1, :, end_positive[j] - length_pos_neg: end_positive[j]\n",
    "            ] for j in range(batch_size)]\n",
    "        ))  # Positive samples representations\n",
    "\n",
    "        size_representation = representation.size(1)\n",
    "        # Positive loss: -logsigmoid of dot product between anchor and positive\n",
    "        # representations\n",
    "        loss = -torch.mean(torch.nn.functional.logsigmoid(torch.bmm(\n",
    "            representation.view(batch_size, 1, size_representation),\n",
    "            positive_representation.view(batch_size, size_representation, 1)\n",
    "        )))\n",
    "\n",
    "        # If required, backward through the first computed term of the loss and\n",
    "        # free from the graph everything related to the positive sample\n",
    "        if save_memory:\n",
    "            loss.backward(retain_graph=True)\n",
    "            loss = 0\n",
    "            del positive_representation\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        multiplicative_ratio = self.negative_penalty / self.nb_random_samples\n",
    "        for i in range(self.nb_random_samples):\n",
    "            # Negative loss: -logsigmoid of minus the dot product between\n",
    "            # anchor and negative representations\n",
    "            negative_representation = encoder(\n",
    "                torch.cat([train[samples[i, j]: samples[i, j] + 1][\n",
    "                    :, :,\n",
    "                    beginning_samples_neg[i, j]:\n",
    "                    beginning_samples_neg[i, j] + length_pos_neg\n",
    "                ] for j in range(batch_size)])\n",
    "            )\n",
    "            loss += multiplicative_ratio * -torch.mean(\n",
    "                torch.nn.functional.logsigmoid(-torch.bmm(\n",
    "                    representation.view(batch_size, 1, size_representation),\n",
    "                    negative_representation.view(\n",
    "                        batch_size, size_representation, 1\n",
    "                    )\n",
    "                ))\n",
    "            )\n",
    "            # If required, backward through the first computed term of the loss\n",
    "            # and free from the graph everything related to the negative sample\n",
    "            # Leaves the last backward pass to the training procedure\n",
    "            if save_memory and i != self.nb_random_samples - 1:\n",
    "                loss.backward(retain_graph=True)\n",
    "                loss = 0\n",
    "                del negative_representation\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_run(data, encoder, device, window_size,batch_size, optimizer=None, train=True):\n",
    "    \n",
    "    comp_len = window_size\n",
    "    if window_size==-1:\n",
    "        comp_len = None\n",
    "        \n",
    "    if train:\n",
    "        encoder.train()\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        \n",
    "    encoder = encoder.to(device)\n",
    "    loss_criterion = TripletLoss(compared_length=comp_len, nb_random_samples=10, negative_penalty=1)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(data).float().to(device),\n",
    "                                             torch.zeros((len(data),1)).to(device))\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    i = 0\n",
    "\n",
    "    for x_batch,y in data_loader:\n",
    "        \n",
    "        loss = loss_criterion(x_batch.to(device), encoder, torch.tensor(data).float().to(device))\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        i += 1\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return epoch_loss/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_encoder(n_cross_val,data_type,datasets,lr,window_size,tr_percentage,batch_size,\n",
    "                  encoder_type,encoding_size,decay,n_epochs,suffix,device,device_ids,verbose,show_encodings):\n",
    "        \n",
    "    accuracies=[]\n",
    "    for cv in range(n_cross_val):\n",
    "        train_data,train_labels,test_data,test_labels = load_datasets(data_type,datasets,cv)\n",
    "\n",
    "        #Save Location\n",
    "        save_dir = './results/baselines/%s_tloss/%s/'%(datasets,data_type)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        save_file = str((save_dir +'encoding_%d_encoder_%d_checkpoint_%d%s.pth.tar')\n",
    "               %(encoding_size,encoder_type, cv,suffix))\n",
    "        \n",
    "        if verbose:\n",
    "            print('Saving at: ',save_file)\n",
    "        \n",
    "        #Models\n",
    "\n",
    "        input_size = train_data.shape[1]\n",
    "        encoder,_ = select_encoder(device,encoder_type,input_size,encoding_size)\n",
    "        \n",
    "        #Training init\n",
    "        params = encoder.parameters()\n",
    "        optimizer = torch.optim.Adam(params, lr=lr, weight_decay=decay)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, n_epochs, gamma=0.999)\n",
    "        \n",
    "        #Split/Shuffle train and val\n",
    "        inds = list(range(len(train_data)))\n",
    "        random.shuffle(inds)\n",
    "        train_data = train_data[inds]\n",
    "        n_train = int(tr_percentage*len(train_data))\n",
    "        best_acc = 0\n",
    "        best_loss = np.inf\n",
    "        train_loss, val_loss = [], []\n",
    "\n",
    "        #Train\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = epoch_run(train_data[:n_train], encoder, device, window_size,batch_size,\n",
    "                                   optimizer=optimizer, train=True)\n",
    "            epoch_loss_val = epoch_run(train_data[n_train:], encoder, device, window_size,batch_size,\n",
    "                                       optimizer=optimizer, train=False)\n",
    "            #scheduler.step()\n",
    "            \n",
    "            if verbose:\n",
    "                print('\\nEpoch ', epoch)\n",
    "                print('Train ===> Loss: ', epoch_loss)\n",
    "                print('Validation ===> Loss: ', epoch_loss_val)\n",
    "            \n",
    "            train_loss.append(epoch_loss)\n",
    "            val_loss.append(epoch_loss_val)\n",
    "            \n",
    "            if epoch_loss_val<best_loss:\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'encoder_state_dict': encoder.state_dict()\n",
    "                }\n",
    "                best_loss = epoch_loss_val\n",
    "                torch.save(state, save_file)\n",
    "                if verbose:\n",
    "                    print('Saving ckpt')\n",
    "                \n",
    "        accuracies.append(best_acc)\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(n_epochs), train_loss, label=\"Train\")\n",
    "        plt.plot(np.arange(n_epochs), val_loss, label=\"Validation\")\n",
    "        plt.title(\"Tloss Unsupervised Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(save_dir +'encoding_%d_encoder_%d_checkpoint_%d%s.png'%(encoding_size,encoder_type, cv,suffix))\n",
    "        if verbose:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        if verbose:\n",
    "            print('Best Train ===> Loss: ', np.min(train_loss))\n",
    "            print('Best Validation ===> Loss: ', np.min(val_loss))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tloss(args):\n",
    "    #Run Process\n",
    "\n",
    "    if args['batch_size']<1:\n",
    "        args['batch_size'] = max(1,int(min(len(args['train_data']),len(test_data))*args['batch_size']))\n",
    "        print('Using batch_size:', args['batch_size'])\n",
    "    \n",
    "    learn_encoder(**args)\n",
    "    \n",
    "    #Plot Features\n",
    "    title = 'Tloss Encoding TSNE for %s'%(args['data_type'])\n",
    "    \n",
    "    if args['show_encodings']:\n",
    "        for cv in range(args['n_cross_val']):\n",
    "            train_data,train_labels,test_data,test_labels = load_datasets(args['data_type'],args['datasets'],cv)\n",
    "            utils_plot.plot_distribution(test_data, test_labels,args['encoder_type'],\n",
    "                                                             args['encoding_size'],args['window_size'],'tloss',\n",
    "                                                             args['datasets'],args['data_type'],args['suffix'],\n",
    "                                                             args['device'], title, cv)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    #Devices\n",
    "    args['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    args['device_ids'] = [i for i in range(torch.cuda.device_count())]\n",
    "    print('Using', args['device'])\n",
    "    \n",
    "    \n",
    "    if args['data_type'] == 'urban':\n",
    "        args['n_cross_val'] = 10\n",
    "        \n",
    "    #Experiment Parameters\n",
    "    args['window_size'] = 2500\n",
    "    args['encoder_type'] = 1\n",
    "    args['encoding_size'] = 128\n",
    "    args['lr'] = 1e-3\n",
    "    args['decay'] = 1e-5\n",
    "    args['datasets'] = args['data_type']\n",
    "\n",
    "    run_tloss(args)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = {'n_cross_val':5,\n",
    "        'data_type':'afdb', #options: afdb, ims, urban\n",
    "        'tr_percentage':0.8,\n",
    "        'n_epochs':100,\n",
    "        'suffix':'',\n",
    "        'batch_size':8,\n",
    "        'show_encodings':False,\n",
    "        'verbose': True} \n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
