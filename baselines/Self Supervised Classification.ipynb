{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,balanced_accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "os.chdir(\"../\") #Load from parent directory\n",
    "from data_utils import Plots,gen_loader,load_datasets,compute_avg,log_data\n",
    "from models import select_encoder\n",
    "utils_plot=Plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised(n_cross_val,data_type,model_name,tr_percentage,val_percentage,n_epochs,ablation,classifier_type,batch_size,\n",
    "                     suffix,device,device_ids,window_size,encoder_type,encoding_size,lr,decay,\n",
    "                     datasets,show_encodings,n_classes,verbose):\n",
    "    res = {}\n",
    "    train_accs, test_accs = {},{}\n",
    "    train_losses, test_losses = {},{}\n",
    "    val_accs,val_losses = {},{}\n",
    "    test_aucs = {}\n",
    "    \n",
    "    for cv in range(n_cross_val):\n",
    "        #Data\n",
    "        train_loader,val_loader,test_loader = gen_loader(data_type,datasets,n_classes,tr_percentage,\n",
    "                                                         val_percentage,window_size,batch_size,cv)\n",
    "\n",
    "        #Load Location\n",
    "        load_dir = './results/baselines/%s_%s/%s/'%(datasets,model_name,data_type)\n",
    "            \n",
    "        load_weights = str((load_dir +'encoding_%d_encoder_%d_checkpoint_%d%s.pth.tar')\n",
    "               %(encoding_size,encoder_type, cv,suffix))\n",
    "        \n",
    "        if verbose:\n",
    "            print('Loading from: ',load_weights)\n",
    "        \n",
    "        #Models\n",
    "        input_size = [x.shape for (x,y) in train_loader][0][1]\n",
    "        encoder,classifier = select_encoder(device,encoder_type,input_size,encoding_size,n_classes,classifier_type)\n",
    "        encoder = encoder.to(device)\n",
    "        classifier = classifier.to(device)\n",
    "        \n",
    "        checkpoint = torch.load(load_weights)\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        \n",
    "        best_acc = 0\n",
    "        train_accs[cv], test_accs[cv],val_accs[cv] = [],[],[]\n",
    "        train_losses[cv], test_losses[cv],val_losses[cv] =[],[],[]\n",
    "        test_aucs[cv] =[]\n",
    "        \n",
    "        #Define Optimizer and Loss\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        params = classifier.parameters()\n",
    "        optimizer = torch.optim.Adam(params, lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, n_epochs, gamma=0.99)\n",
    "\n",
    "        for e in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for i,(x,y) in enumerate(train_loader):\n",
    "                encoder.eval()\n",
    "                classifier.train()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                encodings = encoder(x.to(device))\n",
    "                prediction = classifier(encodings.detach())\n",
    "                \n",
    "                state_prediction = torch.argmax(prediction, dim=1)\n",
    "\n",
    "                loss = loss_fn(prediction, y.long().to(device))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_acc += torch.eq(state_prediction.to('cpu'), y).sum().item()/len(x)\n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "            scheduler.step()\n",
    "            \n",
    "            if verbose:\n",
    "                print('CV ',cv,' Epoch ',e,'Train Labels',tr_percentage)\n",
    "            \n",
    "            #Training Results\n",
    "            train_loss,train_acc = epoch_loss / batch_count, epoch_acc / batch_count\n",
    "            train_accs[cv].append(train_acc)\n",
    "            train_losses[cv].append(train_loss)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Train Results: ',train_loss,train_acc)\n",
    "                \n",
    "            model=torch.nn.Sequential(encoder, classifier).to(device)\n",
    "            val_loss,val_acc,_ = test_supervised(model,device,val_loader)\n",
    "            val_accs[cv].append(val_acc)\n",
    "            val_losses[cv].append(val_loss)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Validation Results: ',val_loss,val_acc)\n",
    "            \n",
    "            test_loss,test_acc,test_auc = test_supervised(model,device,test_loader,calc_auc=not(ablation))\n",
    "            test_accs[cv].append(test_acc)\n",
    "            test_losses[cv].append(test_loss)\n",
    "            test_aucs[cv].append(test_auc)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Test Results: ',test_loss,test_acc,test_auc)\n",
    "                print('')\n",
    "\n",
    "    res['train_accs'] = train_accs\n",
    "    res['train_losses'] = train_losses\n",
    "    res['val_accs'] = val_accs\n",
    "    res['val_losses'] = val_losses\n",
    "    res['test_accs'] = test_accs\n",
    "    res['test_losses'] = test_losses\n",
    "    res['test_aucs'] = test_aucs\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def test_supervised(model,device,data_loader,calc_auc=False):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_auc = 0\n",
    "    batch_count = 0\n",
    "    y_all, prediction_all = [], []\n",
    "    \n",
    "    for x, y in data_loader:\n",
    "        #print(x.shape)\n",
    "        prediction = model(x.to(device))\n",
    "        state_prediction = torch.argmax(prediction, -1)\n",
    "        loss = loss_fn(prediction, y.long().to(device))\n",
    "        \n",
    "        \n",
    "        epoch_acc += torch.eq(state_prediction.to('cpu'), y).sum().item()/len(x)\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "        y_all.append(y)\n",
    "        prediction_all.append(prediction.detach().cpu().numpy())\n",
    "\n",
    "    if calc_auc:\n",
    "        y_all = np.concatenate(y_all, 0)\n",
    "        prediction_all = np.concatenate(prediction_all, 0)\n",
    "\n",
    "        prediction_class_all = np.argmax(prediction_all, -1)\n",
    "        y_onehot_all = np.zeros(prediction_all.shape)\n",
    "        y_onehot_all[np.arange(len(y_onehot_all)), y_all.astype(int)] = 1\n",
    "        epoch_auc = roc_auc_score(y_onehot_all,prediction_all,multi_class='ovo')\n",
    "        \n",
    "    return epoch_loss / batch_count, epoch_acc / batch_count , epoch_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_res(args,res):\n",
    "    if args['ablation']:\n",
    "        name = './results/baselines/%s_%s/%s/labs_ssl_encoding_%d_encoder_%d_classifier_%d%s'%(\n",
    "            args['datasets'],args['model_name'],args['data_type'],args['encoding_size'],args['encoder_type'],\n",
    "            args['classifier_type'],args['suffix'])\n",
    "        \n",
    "        arr = [res['train_accs_dict'],res['train_losses_dict'],res['val_accs_dict'],res['val_losses_dict'],\n",
    "               res['test_accs_dict'],res['test_losses_dict']]\n",
    "        \n",
    "        names = ['train_accs_dict','train_losses_dict','val_accs_dict','val_losses_dict',\n",
    "         'test_accs_dict','test_losses_dict']\n",
    "    else:\n",
    "        \n",
    "        name = './results/baselines/%s_%s/%s/log_ssl_encoding_%d_encoder_%d_classifier_%d%s'%(\n",
    "            args['datasets'],args['model_name'],args['data_type'],args['encoding_size'],args['encoder_type'],\n",
    "            args['classifier_type'],args['suffix'])\n",
    "        \n",
    "        arr = [res['train_accs'],res['train_losses'],res['val_accs'],res['val_losses'],res['test_accs'],res['test_losses']]\n",
    "        names = ['train_accs','train_losses','val_accs','val_losses','test_accs','test_losses']\n",
    "    \n",
    "    log_data(name,arr,names)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sup(args):\n",
    "    #Run Process\n",
    "    res = train_supervised(**args)\n",
    "    \n",
    "    if args['show_encodings']:\n",
    "        \n",
    "        #Plot Accuracy/Loss\n",
    "        utils_plot.plot_acc_loss('Fully Supervised',compute_avg(res['train_accs']),compute_avg(res['test_accs']),\n",
    "                      compute_avg(res['train_losses']),compute_avg(res['test_losses']))\n",
    "\n",
    "        #Plot Features\n",
    "        \n",
    "        title = 'Fully Supervised Encoding TSNE for %s'%(args['data_type'])\n",
    "        for cv in range(args['n_cross_val']):\n",
    "            train_data,train_labels,test_data,test_labels = load_datasets(args['data_type'],args['datasets'],cv)\n",
    "            utils_plot.plot_distribution(test_data, test_labels,args['encoder_type'],\n",
    "                                         args['encoding_size'],args['window_size'],'sup',\n",
    "                                         args['datasets'],args['data_type'],args['suffix'],\n",
    "                                         args['device'], title, cv)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(args,res):\n",
    "    metrics = {}\n",
    "    max_acc = np.mean(np.max(np.array(list(res['test_accs'].values())),axis=1).flatten())\n",
    "    \n",
    "    #Calculate Final Test Accuracy using highest validation accuracy\n",
    "    ids = [np.argmax(res['val_accs'][i]) for i in range(args['n_cross_val'])]\n",
    "    nums = [res['test_accs'][i][ids[i]] for i in range(args['n_cross_val'])]\n",
    "    \n",
    "    final_acc = np.mean(nums)\n",
    "    final_diff = max((np.mean(nums) - min(nums)),(max(nums)-np.mean(nums)))\n",
    "            \n",
    "    ids = [np.argmax(res['val_accs'][i]) for i in range(args['n_cross_val'])]\n",
    "    nums = [res['test_aucs'][i][ids[i]] for i in range(args['n_cross_val'])]\n",
    "    \n",
    "    final_auc = np.mean(nums)\n",
    "    final_auc_diff = max((np.mean(nums) - min(nums)),(max(nums)-np.mean(nums)))\n",
    "    \n",
    "    metrics['final_acc'] = final_acc\n",
    "    metrics['final_diff'] = final_diff\n",
    "    metrics['final_auc'] = final_auc\n",
    "    metrics['final_auc_diff'] = final_auc_diff\n",
    "    metrics['max_acc'] = max_acc\n",
    "    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    #Devices\n",
    "    args['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    args['device_ids'] = [i for i in range(torch.cuda.device_count())]\n",
    "    print('Using', args['device'])\n",
    "\n",
    "    #Experiments\n",
    "    if args['data_type']=='afdb':\n",
    "        args['n_classes'] = 4\n",
    "    elif args['data_type']=='ims':\n",
    "        args['n_classes'] = 5\n",
    "    elif args['data_type']=='urban':\n",
    "        args['n_cross_val'] = 10\n",
    "        args['n_classes'] = 10 \n",
    "        \n",
    "    #Experiment Parameters\n",
    "    args['window_size'] = 2500\n",
    "    args['encoder_type'] = 1\n",
    "    args['encoding_size'] = 128\n",
    "    args['lr'] = 1e-3\n",
    "    args['decay'] = 1e-5\n",
    "    args['datasets'] = args['data_type']\n",
    "    args['n_classes'] = 10\n",
    "        \n",
    "    if args['ablation'] == False:\n",
    "        results = run_sup(args)\n",
    "        save_res(args,results)\n",
    "\n",
    "        metrics = calc_metrics(args,results)\n",
    "        print(metrics)\n",
    "\n",
    "    else:\n",
    "        train_accs_dict,train_losses_dict = {},{}\n",
    "        test_accs_dict,test_losses_dict = {},{}\n",
    "        val_accs_dict,val_losses_dict = {},{}\n",
    "        args['show_encodings'] = False\n",
    "        tr = [0.01,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n",
    "\n",
    "        for train_per in tr:\n",
    "            args['tr_percentage']= train_per\n",
    "            results = run_sup(args)\n",
    "\n",
    "            train_accs_dict[train_per] = results['train_accs']\n",
    "            train_losses_dict[train_per] = results['train_losses']\n",
    "            test_accs_dict[train_per] = results['test_accs']\n",
    "            test_losses_dict[train_per] = results['test_losses']\n",
    "            val_accs_dict[train_per] = results['val_accs']\n",
    "            val_losses_dict[train_per] = results['val_losses']\n",
    "\n",
    "            results['train_accs_dict'] = train_accs_dict\n",
    "            results['train_losses_dict'] = train_losses_dict\n",
    "            results['test_accs_dict'] = test_accs_dict\n",
    "            results['test_losses_dict'] = test_losses_dict\n",
    "            results['val_accs_dict'] = val_accs_dict\n",
    "            results['val_losses_dict'] = val_losses_dict\n",
    "\n",
    "        save_res(args,results)\n",
    "        utils_plot.plot_ablation(args['n_cross_val'],tr,val_accs_dict,test_accs_dict,test_losses_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = {'n_cross_val':10,\n",
    "        'model_name':'cpc', #options: cpc, tloss, tnc, simclr\n",
    "        'data_type':'afdb', #options: afdb, ims, urban\n",
    "        'tr_percentage':0.8,\n",
    "        'val_percentage': 0.2,\n",
    "        'n_epochs':15,\n",
    "        'ablation': False,\n",
    "        'classifier_type':0,\n",
    "        'batch_size': 100,\n",
    "        'suffix':'',\n",
    "        'verbose':True,\n",
    "        'show_encodings': False} \n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
